DevOps Real-time Project (for Freshers)
Create EKS Cluster Using Cloudformation Templates Stored in an S3 Bucket 
Also,
Troubleshooting with Shell Scripting !

This approach allows you to manage your infrastructure as code (IaC) and reuse templates for consistent deployments.
Pre-requisite
1.	Create Bastion EC2 Instance:
Follow the steps for setting up a bastion host in your VPC to access private resources securely.
2.	Install eksctl, kubectl, awscli:
sudo apt update && sudo apt -y full-upgrade
sudo apt install unzip -y

# AWS CLI installation
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
rm -fr awscliv2.zip aws 

# kubectl installation
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
rm -f kubectl

# eksctl installation
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
sudo mv /tmp/eksctl /usr/local/bin
3.	Configure AWS CLI:
aws configure
4.	Create IAM User:
o	Set up the IAM user with proper permissions to interact with AWS services.
________________________________________
Step 1: Prepare CloudFormation Templates
CloudFormation Template Example with IGW and Additional Resources
Create a CloudFormation template that defines the EKS cluster along with VPC, Internet Gateway, subnets, and other necessary resources. Save this as 
vim eks-cluster.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: CloudFormation template to create an EKS cluster

Resources:
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: eks-vpc

  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: us-east-1a
      Tags:
        - Key: Name
          Value: eks-public-subnet-1

  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.2.0/24
      MapPublicIpOnLaunch: true
      AvailabilityZone: us-east-1b
      Tags:
        - Key: Name
          Value: eks-public-subnet-2

  InternetGateway:
    Type: AWS::EC2::InternetGateway

  AttachGateway:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway

  EKSCluster:
    Type: AWS::EKS::Cluster
    Properties:
      Name: my-eks-cluster
      RoleArn: arn:aws:iam::730335391873:role/EKSClusterRole
      ResourcesVpcConfig:
        SubnetIds:
          - !Ref PublicSubnet1
          - !Ref PublicSubnet2

Outputs:
  EKSClusterName:
    Description: Name of the EKS cluster
    Value: !Ref EKSCluster
    Export:
      Name: EKSClusterName

  PublicSubnet1:
    Description: Public Subnet 1
    Value: !Ref PublicSubnet1
    Export:
      Name: PublicSubnet1

  PublicSubnet2:
    Description: Public Subnet 2
    Value: !Ref PublicSubnet2
    Export:
      Name: PublicSubnet2

Validate 
aws cloudformation validate-template --template-body file://eks-cluster.yaml
Upload the Template to S3:
Use the AWS CLI to upload the template:
aws s3 cp eks-cluster.yaml s3://my-bucket/eks-cluster.yaml
________________________________________
Step 2: Create the EKS Cluster Using CloudFormation
1.	Create CloudFormation Stack: Use the AWS CLI to create a stack from the template stored in S3:
aws cloudformation create-stack \
  --stack-name eks-cluster-stack \
  --template-url https://my-bucket.s3.amazonaws.com/eks-cluster.yaml \
  --capabilities CAPABILITY_NAMED_IAM
2.	Monitor Stack Creation: Monitor the stack creation process:
aws cloudformation describe-stacks --stack-name eks-cluster-stack
________________________________________
Step 3: Configure kubectl for the EKS Cluster
1.	Update kubeconfig: Once the stack is created, update your kubeconfig file:
aws eks update-kubeconfig --name my-eks-cluster --region us-east-1
2.	Verify Cluster Access: Verify if you can access the cluster:
kubectl get nodes
________________________________________
Step 4: Deploy Worker Nodes
1.	Create a Node Group: Create a CloudFormation template for a managed node group. The example below is for a NodeGroup configuration:
vim node.yaml
Resources:
  NodeGroup:
    Type: AWS::EKS::Nodegroup
    Properties:
      ClusterName: !Ref EKSCluster
      NodeRole: !GetAtt NodeInstanceRole.Arn
      Subnets:
        - !Ref PublicSubnet1
        - !Ref PublicSubnet2
      ScalingConfig:
        MinSize: 1
        MaxSize: 3
        DesiredSize: 2
      InstanceTypes:
        - t3.medium
      DiskSize: 20

  NodeInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
2.	Upload and Deploy the Node Group Template:
aws s3 cp nodegroup.yaml s3://my-bucket/nodegroup.yaml
aws cloudformation create-stack \
  --stack-name my-eks-nodegroup-stack \
  --template-url https://my-bucket.s3.amazonaws.com/nodegroup.yaml \
  --capabilities CAPABILITY_NAMED_IAM
3.	Verify Worker Nodes: Verify if the worker nodes are registered with the cluster:
kubectl get nodes

Deploy a Sample Application
1.	Create a Deployment:
vim deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

Apply the deployment:
kubectl apply -f nginx-deployment.yaml

2.	Create a Service:
vim service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

Apply the service:
kubectl apply -f nginx-service.yaml

3.	Verify Application Deployment:
kubectl get pods
kubectl get svc

Simulate and Troubleshoot Common Issues
Scenario 1: Pods are Not Running
1.	Simulate Issue:
o	Delete a pod manually:
kubectl delete pod <pod-name>
2.	Troubleshoot:
o	Check pod status:
kubectl get pods
o	Describe the pod for more details:
kubectl describe pod <pod-name>
o	Check logs:
kubectl logs <pod-name>
Scenario 2: Service is Not Accessible
1.	Simulate Issue:
o	Change the service type from LoadBalancer to ClusterIP:
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

2.	Apply the change:
        kubectl apply -f nginx-service.yaml
3.	Troubleshoot:
o	Check service status:
kubectl get svc
o	Describe the service:
kubectl describe svc nginx-service
o	Verify network policies and security groups.
Scenario 3: Nodes are Not Ready
1.	Simulate Issue:
o	Drain a node:
kubectl drain <node> --ignore-daemonsets --delete-emptydir-data
2.	Troubleshoot:
o	Check node status:
kubectl get nodes
o	Describe the node:
kubectl describe node <node-name>
o	Check cloud provider logs and instance status.
Restart node
kubectl uncordon ip-192-168-0-122.ec2.intern
Step 4: Automate Troubleshooting with Scripts
1.	Create a Script to Check Pod Status:
vim pod.sh
#!/bin/bash
POD_STATUS=$(kubectl get pods -o jsonpath='{.items[*].status.phase}')
if [[ $POD_STATUS != "Running" ]]; then
  echo "Some pods are not running"
  kubectl get pods
else
  echo "All pods are running"
fi

sudo chmod +x pod.sh
./pod.sh

2.	Create a Script to Check Node Status:
vim node.sh
#!/bin/bash
NODE_STATUS=$(kubectl get nodes -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}')
if [[ $NODE_STATUS != "True" ]]; then
  echo "Some nodes are not ready"
  kubectl get nodes
else
  echo "All nodes are ready"
fi

sudo chmod +x node.sh
./node.sh

Step 5: Document Findings and Resolutions
1.	Create a Troubleshooting Guide:
o	Document each scenario, the steps to simulate the issue, and the troubleshooting steps.
o	Include screenshots and command outputs.
2.	Create a Runbook:
o	Provide a step-by-step guide for common issues and their resolutions.
o	Include contact information for further support.
Step 6: Clean Up
1.	Delete the EKS Cluster:
eksctl delete cluster --name my-cluster --region us-west-2
2.	Verify Deletion:
kubectl get nodes

Scenario 1: Application Not Responding (502 Bad Gateway)
Simulate the Issue:
1.	Deploy a basic Nginx application: Create a nginx-deployment.yaml file and apply it:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80

kubectl apply -f nginx-deployment.yaml
2.	Create a service to expose the application via LoadBalancer:

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

kubectl apply -f nginx-service.yaml
3.	Delete the pod to simulate a crash:

kubectl delete pod <nginx-pod-name>
4.	Check the LoadBalancer: After some time, try to access the application. You should see a 502 Bad Gateway.
Steps to Troubleshoot:
1.	Check pod status:

kubectl get pods
Look for the CrashLoopBackOff or Error state.
2.	Check pod logs:

kubectl logs <nginx-pod-name>
Logs will show if the container failed to start properly, often due to a misconfiguration.
3.	Check service status:

kubectl describe svc nginx-service
4.	Fix the deployment by re-deploying or scaling the pod:

kubectl scale deployment nginx-deployment --replicas=3
5.	After the new pod is running, access the LoadBalancer IP and verify that the issue is resolved.
________________________________________
Scenario 2: LoadBalancer External IP Not Assigned
Simulate the Issue:
1.	Create a service with LoadBalancer type and wait:

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

kubectl apply -f nginx-service.yaml
2.	Check the external IP:

kubectl get svc nginx-service
The EXTERNAL-IP might stay as pending.
Steps to Troubleshoot:
1.	Check if the LoadBalancer was created:
o	Go to AWS Console → EC2 → Load Balancers and check if a LoadBalancer is created.
o	Check your AWS quota for LoadBalancers.
2.	Check IAM Permissions: Ensure that the EKS node group has the necessary permissions (AmazonEKSWorkerNodePolicy) to create and manage AWS resources like LoadBalancers.
3.	Check VPC Subnet Configuration: Ensure that the subnets used by your nodes are publicly accessible to create LoadBalancers.
4.	Wait for the LoadBalancer to be provisioned. Sometimes it can take several minutes.
________________________________________
Scenario 3: Pods are Pending
Simulate the Issue:
1.	Create a Deployment with a high resource request:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        ports:
        - containerPort: 80

kubectl apply -f nginx-deployment.yaml
2.	Check pod status:
kubectl get pods
The pod may stay in Pending state if there are not enough resources available in the cluster.
Steps to Troubleshoot:
1.	Describe the pod:

kubectl describe pod <nginx-pod-name>
Look for events like Insufficient cpu or Insufficient memory.
2.	Check node resources:
kubectl describe nodes
Ensure the nodes have enough resources to schedule the pods.
3.	Scale deployment or reduce resource requests to avoid resource constraints:
kubectl scale deployment nginx-deployment --replicas=3
4.	If resource constraints are the issue, lower the requests and limits for CPU/memory.
________________________________________
Scenario 4: Service Not Exposing Pods Correctly
Simulate the Issue:
1.	Create a Service with incorrect selector (mismatch between pod labels and service selector):

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: wrong-label
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer

Apply the service file:

kubectl apply -f nginx-service.yaml
2.	Verify service status:

kubectl describe svc nginx-service
The service will show that it cannot expose any pods.
Steps to Troubleshoot:
1.	Verify pod labels:

kubectl get pods --show-labels
Ensure that the pods have the correct labels matching the service selector. If not, update the service or deployment to match the labels.
2.	Fix service selector to match pod labels:

selector:
  app: nginx  # Correct the label mismatch
Apply the corrected service:

kubectl apply -f nginx-service.yaml
________________________________________
Scenario 5: Load Balancer IP Accessible, But Application Fails to Load
Simulate the Issue:
1.	Simulate a misconfigured Nginx app by changing the root directory in the Nginx config:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        command: ["nginx", "-g", "daemon off;"]
        volumeMounts:
          - name: nginx-config
            mountPath: /etc/nginx/nginx.conf
            subPath: nginx.conf
        ports:
        - containerPort: 80
Make sure to apply a broken or incorrect configuration file.
2.	Access the LoadBalancer IP, and it will fail to load.
Steps to Troubleshoot:
1.	Check the pod logs:

kubectl logs <nginx-pod-name>
The logs will show if there are Nginx errors (e.g., "configuration file not found").
2.	Revert the Nginx configuration to the default or correct config, and redeploy the pod.
3.	Check security group settings to ensure the LoadBalancer allows HTTP/HTTPS traffic.
___________________________________
Conclusion
This project provides a comprehensive approach to setting up, simulating issues, and troubleshooting an Amazon EKS cluster. By following these steps, you can gain a deeper understanding of EKS and improve your troubleshooting skills.

Thank You !

